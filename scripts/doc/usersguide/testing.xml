<chapter id="ccsm_tests">
<title>CESM Testing</title>

<!-- ======================================================================= -->
<sect1 id="ccsm_tests_summary">
<title>Testing overview</title>

<para>
CESM1 is accompanied by utilities that support automated testing of
the model.  In general, these should be used only after the model has
been ported to the target machine (see <xref linkend="port"/>).  The
tools
are <command>create_production_test</command>, <command>create_test</command>,
and <command>create_test_suite</command>.  <command>create_production_test</command>
tool is executed from $&CASEROOT;, and tests exact
restartability of the case setup in a separate directory.  
<command>create_test</command> is executed from &CCSMROOT/scripts and
allows you to quickly set up and run one of several supported tests.
Finally, <command>create_test_suite</command> is primarily for CSEG
developers and is there to easily set up and run a list of supported
tests.  Each of these tools will be described below.
</para>

</sect1>

<sect1 id="create_production_test">
<title>create_production_test</title>

<para> In general, after configuring and testing a case and before 
starting a long production job based on that case, it's important
to verify that the model restarts exactly.  This is a standard
requirement of the system and will help demonstrate stability
of the configuration technically.  The tool
create_production_test is located in the case directory,
and it sets up an ERU two month exact restart test in a separate directory
based on the current case.  To use it, do the following
</para>

<screen>
> cd $CASEROOT
> ./create_production_test
> cd ../$CASE_ERT
> $CASE_ERT.test_build
submit $CASE_ERT.test
Check your test results. A successful test produces "PASS" as
the first word in the file, $CASE_ERT/TestStatus
</screen>

<para>If the test fails, see <xref linkend="failed_tests"/> for
test debugging guidance.
</para>

</sect1>

<sect1 id="create_test">
<title>create_test</title>

<para>
The create_test tool is located in the scripts directory and
can be used to set up a standalone test case.  The test cases are
fixed and defined within the CCSM scripts.  To see the list of test cases
or for additional help, type "create_test -help" from the scripts
directory.  To use create_test, do something like
</para>

<screen>
> cd $CCSMROOT/scripts
> ./create_test -testname ERS.f19_g16.X.bluefire_ibm -testid t01
> cd ERS.f19_g16.X.bluefire_ibm.t01
> ERS.f19_g16.X.bluefire_ibm.t01.test_build
submit ERS.f19_g16.X.bluefire.t01.test
Check your test results. A successful test produces "PASS" as
  the first word in the file TestStatus
</screen>

<para>
The above sets up an exact restart test (ERS) at the 1.9x2.5_gx1v6
resolution using a dead model compset (X) for the machine bluefire.
The testid provides a unique tag for the test in case it needs to
be rerun (i.e. using -testid t02).  Some things to note about CCSM tests
</para>


<itemizedlist>
<listitem>
<para>For more information about the create_test tool, run "create_test -help".</para>
</listitem>
<listitem>
<para>Test results are set in the TestStatus file.  The TestStatus.out file 
provides additional details.</para>
</listitem>
<listitem>
<para>Tests are not always easily re-runable from an existing test directory.  Rather
than rerun a previous test case, it's best to set up a clean test case
(i.e. with a new testid).</para>
</listitem>
<listitem>
<para>Tests are built using the .test_build script.  This is different from cases which are
built using the .build script.  Some tests require more than one executable, the .test_build
script builds all required executables upfront interactively.</para>
</listitem>
<listitem>
<para>The costs of tests vary widely.  Some are short and some are long.</para>
</listitem>
<listitem>
<para>If a test fails, see <xref linkend="failed_tests"/>.</para>
</listitem>
<listitem>
<para>There are -compare and -generate options for the create_test tool that support
regression testing.</para>
</listitem>
<listitem>
<para>There are extra test options that can be added to the test such as _D, _E, or
_P*.  These are described in more detail in the create_test -help output.</para>
</listitem>
</itemizedlist>

<para>The test status results have the following meaning
</para>

<informaltable>
<tgroup cols="2">
<thead>
<row><entry>Test Result</entry><entry>Description</entry></row>
</thead>
<tbody>
<row><entry> BFAIL </entry><entry>  compare test couldn't find base result</entry></row>
<row><entry> BUILD </entry><entry>  build succeeded, test not submitted</entry></row>
<row><entry> CFAIL </entry><entry>  env variable or build error</entry></row>
<row><entry> CHECK </entry><entry>  manual review of data is required</entry></row>
<row><entry> ERROR </entry><entry>  test checker failed, test may or may not have passed</entry></row>
<row><entry> FAIL  </entry><entry>  test failed</entry></row>
<row><entry> GEN   </entry><entry>  test has been generated</entry></row>
<row><entry> PASS  </entry><entry>  test passed</entry></row>
<row><entry> PEND  </entry><entry>  test has been submitted</entry></row>
<row><entry> RUN   </entry><entry>  test is currently running OR it hung, timed out, or exitied ungracefully </entry></row>
<row><entry> SFAIL </entry><entry>  generation of test failed in scripts</entry></row>
<row><entry> TFAIL </entry><entry>  test setup error</entry></row>
<row><entry> UNDEF </entry><entry>  undefined result</entry></row>
</tbody>
</tgroup>
</informaltable>

<para>
The following tests are available at the time of writing
</para>

<informaltable>
<tgroup cols="2">
<thead>
<row><entry>Test</entry><entry>Description</entry></row>
</thead>
<tbody>
<row><entry>SMS </entry><entry> smoke test </entry></row>
<row><entry>ERS </entry><entry> exact restart from startup, default 6 days + 5 days </entry></row>
<row><entry>ERB </entry><entry> branch/exact restart test </entry></row>
<row><entry>ERH </entry><entry> hybrid/exact restart test </entry></row>
<row><entry>ERI </entry><entry> hybrid/branch/exact restart test</entry></row>
<row><entry>ERT </entry><entry> exact restart from startup, default 2 months + 1 month </entry></row>
<row><entry>SEQ </entry><entry> sequencing bit-for-bit test </entry></row>
<row><entry>PEA </entry><entry> single processor testing with mpi and mpi-serial</entry></row>
<row><entry>PEM </entry><entry> pe counts mpi bit-for-bit test </entry></row>
<row><entry>PET </entry><entry> pe counts mpi/openmp bit-for-bit test</entry></row>
<row><entry>CME </entry><entry> compare mct and esmf interfaces test</entry></row>
<row><entry>NCK </entry><entry> single vs multi instance validation test</entry></row>
</tbody>
</tgroup>
</informaltable>

</sect1>

<sect1 id="create_test_suite">
<title>create_test_suite</title>

<para>
The create_test_suite tool is located in the scripts directory and
can be used to set up a suite of standalone test cases automatically.
To use this tool, a list of tests needs to exist in a file.  Some
examples can be found in the directory scripts/ccsm_utils/Testlists.
create_test_suite in invoked on a list of tests and then the full
list of tests is generated.  In addition, an automated submission
script and reporting script are created.  The cs.submit script reduces
the time to submit multiple test cases significantly.  To use this tool, 
do something like the following
</para>

<screen>
create a list of desired tests in some filename, i.e. my_lists
> create_test_suite -input_list my_lists -testid suite01
> ./cs.status.suite01
> ./cs.submit.suite01.$MACH
> ./cs.status.suite01
</screen>

<para>
The cs.status script is generated by create_test_suite, and it reports
the status of all the tests in the suite.  The cs.submit script builds
and submits all the tests sequentially.  The cs.submit script should only
be executed once to build and submit all the tests.  The cs.status script
can be executed as often as needed to check the status of the tests.
When all the tests have completed running, then the results are static
and complete.  To help debug failed tests, see <xref linkend="failed_tests"/>..
</para>

</sect1>

<sect1 id="failed_tests">
<title>Debugging Tests That Fail</title>

<para>
This section describes what steps can be taken to try to identify
why a test failed.  The primary information associated with reviewing
and debugging a run can be found in <xref linkend="troubleshooting_run_time"/>.
</para>

<para>
First, verify that a test case is no longer in the batch queue.  If that's
the case, then review the <link linkend="create_test">possible test results</link>
and compare that to the result in the TestStatus file.  Next, review the 
TestStatus.out file to see if there is any additional information about what
the test did.  Finally, go to the <link linkend="troubleshooting_run_time">
troubleshooting</link> section and work through the various log files.
</para>

<para>
Finally, there are a couple other things to mention.  If the TestStatus file contains 
"RUN" but the job is no longer in the queue, it's possible that the job
either timed out because the wall clock on the batch submission was too
short, or the job hung due to some run-time error.  Check the batch log files
to see if the job was killed due to a time limit, and if it was increase
the time limit and either resubmit the job or generate a new test case
and update the time limit before submitting it.  </para>

<para>Also, a test case can fail
because either the job didn't run properly or because the test conditions
(i.e. exact restart) weren't met.  Try to determine whether the test failed
because the run failed or because the test did not meet the test conditions.
If a test is failing early in a run, it's usually best to set up a
standalone case with the same configuration in order to debug problems.
If the test is running fine, but the test conditions are not being met
(i.e. exact restart), then that requires debugging of the model in the
context of the test conditions.
</para>

<para>
Not all tests will pass for all model configurations.  Some of the issues
we are aware of are
</para>

<itemizedlist>
<listitem>
<para>All models are bit-for-bit reproducible with different processor counts
EXCEPT pop.  The BFBFLAG must be set to TRUE in the &env_run.xml; file if the
coupler is to meet this condition.  There will be a performance penalty when
this flag is set.</para>
</listitem>
<listitem>
<para>Some of the active components cannot run with the mpi serial
library.  This library takes the place of mpi calls when the model is
running on one processors and MPI is not available or not desirable.
The mpi serial library is invoked by setting the xml variable MPILIBS
to mpiserial in &env_build.xml;.  An effort is underway to extend the
mpi serial library to support all components' usage of the mpi library
with this standalone implementation. Also NOT all machines/platforms
are set up to enable setting MPILIBS to mpiserial.  See the summary of
<ulink url="../modelnl/machines.html">supported machines</ulink> for
details.  To use the mpi serial feature on your machine, you also need
to make changes in the Macros and env_machopts files for that
machine. The best way to do this is to use a machine where MPILIBS can
be set to mpiserial and look at the type of
changes needed to make it work. Those same changes will need to be
introduced for your machine. For the Macros file this includes the
name of the compiler, possibly options to the compiler, and the
settings of the MPI library and include path. For the env_machopts
file you may want/need to modify the setting of MPICH_PATH. There also
maybe many settings of MPI specific environment variables that don't
matter when the mpiserial setting is used.</para>
</listitem>
</itemizedlist>

</sect1>

</chapter>


